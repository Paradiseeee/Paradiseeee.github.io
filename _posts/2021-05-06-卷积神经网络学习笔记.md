---
layout:     post
title:      "卷积神经网络学习笔记"
subtitle:   "CNN 的结构层次和定义"
date:       2021-05-06 12:00:00
author:     "Paradise"
header-style: text
tags:
    - 神经网络
    - 计算机视觉
    - 笔记
---

# **两篇相关文章：**

- ResNet残差网络 <https://mp.weixin.qq.com/s?__biz=MzU0ODczMTEwOQ>

- 卷积神经网络数学原理 <https://mp.weixin.qq.com/s/xS34P1fJjf13MWSXjpurzw>

# **学习记录：**

## 1. 卷积神经网络基础单元

### 1.1 卷积（convolution）

- 卷积的计算方式
    - <http://deeplearning.stanford.edu/wiki/index.php/>
    - 或参考第二篇文章，有很简明的描述；
    ![](/post-assets/20210506/20190506221828174.gif)
- 卷积核与通道、分辨率的关系
    - 每个卷积核覆盖全部输入通道；
    - 每个卷积核的结果为一个输出通道；
    - 根据padding，可选地改变分辨率；
- 卷积的作用和设计思路
    - 滤波（kernel-size， stride）；
    - 局部相关性（kernel-size）；
    - 中心感受野（padding， stride）；

### 1.2 激活函数（activation）

- 参考知乎文章：<https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit>
- 激活函数的设计思想：非线性扭曲

![](/post-assets/20210506/20190506221545401.jpg)

- 常见激活函数
    - Sigmoid
    - ReLU/PReLU/SeLu
    - Tanh

### 1.3 池化（pooling）

- 原理：<http://deeplearning.stanford.edu/wiki/index.php/%E6%B1%A0%E5%8C%96>
- 覆盖所有通道，但不改变通道数量；降低分辨率，减少信息量；
- 作用：信息保留（avg_pool）与过滤（ max_pool）；
- 平移不变性（stride）；

### 1.4 批归一化（batch normalization）

- 原理：<https://blog.csdn.net/shuzfan/article/details/50723877>
- 归一化为0均值和单位方差，有利于加速收敛；
- 对应的激活函数：sigmoid、ReLU；

### 1.5 Dropout
- 原理：<https://zhuanlan.zhihu.com/p/21560667>
- 作用：增加随机扰动，减少过拟合，代价是收敛变慢；
        
## 2. 优秀的模型
- [ALexNet](https://blog.csdn.net/luoluonuoyasuolong/article/details/81750190)
- [VGG](https://zhuanlan.zhihu.com/p/41423739)
- [ResNet](https://blog.csdn.net/weixin_43624538/article/details/85049699)
- [DenseNet](https://blog.csdn.net/sigai_csdn/article/details/82115254)
- [Inception](https://www.cnblogs.com/dengshunge/p/10808191.html)
